## 2-1. 훈련 세트와 테스트 세트 ##
### 지도 학습과 비지도 학습 ###
데이터 = 입력, 정답 = 타깃, 이 둘을 합쳐 **훈련 데이터**
입력으로 사용된 길이와 무게 **특성**
지도 학습 = 정답이 있으니 알고리즘이 정답 맞히는 것 학습
비지도 학습 = 타깃 없이 입력 데이터만 사용

### 훈련 세트와 테스트 세트 ###
테스트 세트 = 평가에 사용하는 데이터, 훈련 세트 = 훈련에 사용되는 데이터

#### 생선 예시 ####
샘플 = 하나의 생선 데이터
특성 = 길이와 무게
도미와 빙어 각각 35마리, 14마리인데 처음 35개를 훈련 세트, 나머지 14개를 테스트 세트
리스트처럼 배열 요소 선택시 배열의 위치인 인덱스 지정
슬라이싱 사용시 마지막 인덱스의 원소는 포함되지 않음

### 샘플링 편향 ###
샘플링 편향 = 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않는 것

### 넘파이 ###
고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구 많이 제공
1차 = 선, 2차 = 면, 3차 = 3차원 공간
타겟과 샘플 함께 이동
배열 인덱싱 = 여러 개의 인덱스로 한 번에 여러 개의 원소를 선택

### 두 번째 머신러닝 프로그램 ###
훈련 세트와 테스트 세트로 k-최근접 이웃 모델 훈련. 
fit()메서드 실행 시 KNeighborsClassifer 클래스의 객체는 이전 학습한 것을 잃어 버림

### 훈련 모델 평가 ###
도미와 빙어 섞기 위해 파이썬의 넘파이 사용. 
넘파이는 파이썬의 리스트와 비슷하지만 고차원의 큰 배열을 효과적으로 다룰 수 있고 다양한 많은 도구 사용
shuffle 함수 통해 배열 인덱스 섞었음


## 2-2. 데이터 전처리 ##
데이터가 클수록 넘파이 배열을 사용하는 것이 좋음

### 사이킷런으로 훈련 세트와 테스트 세트 나누기 ###
train_test_split 함수 통해 비율에 맞게 훈련과 테스트 세트로 나눠줌 + 잘 섞어줌
이 때 올바른 비율로 섞이지 않을 수 있어 stratify 매개변수에 타깃 데이터 전달하면 클래스 비율에 맞게 데이터 나눔

### 수상한 도미 한 마리 ###
새로운 샘플은 marker 매개변수를 ^ 지정하여 삼각형으로 나타내면 구분하기 쉬움
k-최근접 이웃은 주변 샘플 중에서 다수인 클래스를 예측으로 사용. 기본 값은 5
marker를 D로 지정하면 산점도를 마름모로 그림

### 기준 맞춰라 ###
X축 범위 지정하려면 xlim 함수 사용, y도 마찬가지
데이터 전처리 = 특성값을 일정한 기준으로 맞춤
가장 흔한 전처리 방법 = 표준점수로 평균 빼고 표준편차 나눠주면 됨
브로드캐스팅이란 크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능

### 전처리 데이터로 모델 훈련하기 ###
샘플을 동일한 비율로 변환해야 함
테스트 세트와 훈련 세트 모두 평균과 표준편차로 변환

### 스케일이 다른 특성 처리 ###
특성의 스케일이 같아야 함
